{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![MLU Logo](../data/MLU_Logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name=\"0\">Machine Learning Accelerator - Natural Language Processing - Lecture 3</a>\n",
    "\n",
    "## Recurrent Neural Networks (RNNs) for the Product Review Problem - Classify Product Reviews as Positive or Not\n",
    "\n",
    "In this exercise, we will learn how to use Recurrent Neural Networks. \n",
    "\n",
    "We will follow these steps:\n",
    "1. <a href=\"#1\">Reading the dataset</a>\n",
    "2. <a href=\"#2\">Exploratory data analysis</a>\n",
    "3. <a href=\"#3\">Train-validation dataset split</a>\n",
    "4. <a href=\"#4\">Text processing and transformation</a>\n",
    "5. <a href=\"#5\">Using GloVe Word Embeddings</a>\n",
    "6. <a href=\"#6\">Training and validating model</a>\n",
    "7. <a href=\"#7\">Improvement ideas</a>\n",
    "\n",
    "Overall dataset schema:\n",
    "* __reviewText:__ Text of the review\n",
    "* __summary:__ Summary of the review\n",
    "* __verified:__ Whether the purchase was verified (True or False)\n",
    "* __time:__ UNIX timestamp for the review\n",
    "* __log_votes:__ Logarithm-adjusted votes log(1+votes)\n",
    "* __isPositive:__ Whether the review is positive or negative (1 or 0)\n",
    "\n",
    "__Important note:__ One big distinction betweeen the regular neural networks and RNNs is that RNNs work with sequential data. In our case, RNNs will help us with the text field. If we also want to consider other fields such as time, log_votes, verified, etc. , we need to use the regular neural networks with the RNN network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install -q -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. <a name=\"1\">Reading the dataset</a>\n",
    "(<a href=\"#0\">Go to top</a>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's read the dataset below and fill-in the reviewText field. We will use this field as input to our ML model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('../data/examples/AMAZON-REVIEW-DATA-CLASSIFICATION.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the first five rows in the dataset. As you can see the __log_votes__ field is numeric. That's why we will build a regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. <a name=\"2\">Exploratory Data Analysis</a>\n",
    "(<a href=\"#0\">Go to top</a>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the range and distribution of log_votes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[\"isPositive\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the number of missing values for each columm below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(df.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have missing values in our text fields."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. <a name=\"3\">Train-validation split</a>\n",
    "(<a href=\"#0\">Go to top</a>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's split the dataset into training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This separates 15% of the entire dataset into validation dataset.\n",
    "train_text, val_text, train_label, val_label = \\\n",
    "    train_test_split(df[\"reviewText\"].tolist(),\n",
    "                     df[\"isPositive\"].tolist(),\n",
    "                     test_size=0.10,\n",
    "                     shuffle=True,\n",
    "                     random_state=324)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. <a name=\"4\">Text processing and Transformation</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "We will apply the following processes here:\n",
    "* __Text cleaning:__ Simple text cleaning operations. We won't do stemming or lemmatization as our word vectors already cover different forms of words. We are using GloVe word embeddings for 6 billion words, phrases or punctuations in this example.\n",
    "* __Tokenization:__ Tokenizing all sentences\n",
    "* __Creating vocabulary:__ We will create a vocabulary of the tokens. In this vocabulary, tokens will map to unique ids, such as \"car\"->32, \"house\"->651, etc.\n",
    "* __Transforming text:__ Tokenized sentences will be mapped to unique ids. For example: [\"this\", \"is\", \"sentence\"] -> [13, 54, 412]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "def cleanStr(text):\n",
    "    \n",
    "    # Check if the sentence is a missing value\n",
    "    if isinstance(text, str) == False:\n",
    "        text = \"\"\n",
    "            \n",
    "    # Remove leading/trailing whitespace\n",
    "    text = text.lower().strip()\n",
    "    # Remove extra space and tabs\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    # Remove HTML tags/markups\n",
    "    text = re.compile('<.*?>').sub('', text)\n",
    "    return text\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = []\n",
    "    text = cleanStr(text)\n",
    "    words = word_tokenize(text)\n",
    "    for word in words:\n",
    "        tokens.append(word)\n",
    "    return tokens\n",
    "\n",
    "def createVocabulary(text_list, min_freq):\n",
    "    all_tokens = []\n",
    "    for sentence in text_list:\n",
    "        all_tokens += tokenize(sentence)\n",
    "    # Calculate token frequencies\n",
    "    counter = Counter(all_tokens)\n",
    "    # Create the vocabulary\n",
    "    vocab = {'<unk>': 0}\n",
    "    idx = 1\n",
    "    for token, count in counter.items():\n",
    "        if count >= min_freq:\n",
    "            vocab[token] = idx\n",
    "            idx += 1\n",
    "    \n",
    "    return vocab\n",
    "\n",
    "def transformText(text, vocab, max_length):\n",
    "    token_arr = np.zeros((max_length,))\n",
    "    tokens = tokenize(text)[0:max_length]\n",
    "    for idx, token in enumerate(tokens):\n",
    "        token_arr[idx] = vocab.get(token, 0)  # Use 0 for unknown words\n",
    "    return token_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to keep the training time low, we only consider the first 250 words (max_length) in sentences. We also only use words that occur more than 5 times in the all sentences (min_freq)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "min_freq = 5\n",
    "max_length = 250\n",
    "\n",
    "print(\"Creating the vocabulary\")\n",
    "vocab = createVocabulary(train_text, min_freq)\n",
    "print(\"Transforming training texts\")\n",
    "train_text_transformed = torch.tensor([transformText(text, vocab, max_length) for text in train_text])\n",
    "print(\"Transforming validation texts\")\n",
    "val_text_transformed = torch.tensor([transformText(text, vocab, max_length) for text in val_text])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see some unique ids for some words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Vocabulary index for computer:\", vocab.get('computer', 0))\n",
    "print(\"Vocabulary index for beautiful:\", vocab.get('beautiful', 0))\n",
    "print(\"Vocabulary index for code:\", vocab.get('code', 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. <a name=\"5\">Using pre-trained GloVe Word Embeddings</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "In this example, we will use GloVe word vectors. `'glove.6B.50d.txt'` file gives us 6 billion words/phrases vectors. Each word vector has 50 numbers in it. The following code shows how to get the word vectors and create an embedding matrix from them. We will connect our vocabulary indexes to the GloVe embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Download and extract Glove\n",
    "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "!unzip glove*.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_glove(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        words = set()\n",
    "        word_to_vec_map = {}\n",
    "        for line in f:\n",
    "            line = line.strip().split()\n",
    "            curr_word = line[0]\n",
    "            words.add(curr_word)\n",
    "            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n",
    "    return words, word_to_vec_map\n",
    "\n",
    "words, word_to_vec_map = load_glove('glove.6B.50d.txt')\n",
    "\n",
    "embedding_dim = 50\n",
    "embedding_matrix = np.zeros((len(vocab), embedding_dim))\n",
    "for word, i in vocab.items():\n",
    "    embedding_vector = word_to_vec_map.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "embedding_matrix = torch.FloatTensor(embedding_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. <a name=\"6\">Training and validation</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "We have processed our text data and also created our embedding matrixes from GloVe. Now, it is time to start the training process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will set our parameters below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Size of the state vectors\n",
    "hidden_size = 12\n",
    "\n",
    "# General NN training parameters\n",
    "learning_rate = 0.01\n",
    "epochs = 15\n",
    "batch_size = 32\n",
    "\n",
    "# Embedding vector and vocabulary sizes\n",
    "num_embed = 50 # glove.6B.50d.txt\n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to put our data into correct format before the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "train_label = torch.tensor(train_label)\n",
    "val_label = torch.tensor(val_label)\n",
    "\n",
    "train_dataset = TensorDataset(torch.tensor(train_text_transformed), train_label)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our sequential model is made of these layers:\n",
    "* Embedding layer: This is where our words/tokens are mapped to word vectors.\n",
    "* RNN layer: We will be using a simple RNN model. We won't stack RNN units in this example. It uses a single RNN unit with its hidden state size of 12.\n",
    "* Linear layer: A linear layer with a single neuron is used to output our log_votes prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class SimpleRNN(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, num_embed, hidden_size):\n",
    "        super(SimpleRNN, self).__init__()\n",
    "        self.embedding = torch.nn.Embedding(vocab_size, num_embed)\n",
    "        self.rnn = torch.nn.RNN(num_embed, hidden_size, batch_first=True)\n",
    "        self.fc = torch.nn.Linear(hidden_size, 1)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        output, _ = self.rnn(embedded)\n",
    "        output = self.fc(output[:, -1, :])\n",
    "        return self.sigmoid(output)\n",
    "\n",
    "model = SimpleRNN(vocab_size, num_embed, hidden_size).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's initialize this network. Then, we will need to make the embedding layer use our GloVe word vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize networks parameters\n",
    "def init_weights(m):\n",
    "    if type(m) == torch.nn.Linear:\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "\n",
    "model.apply(init_weights)\n",
    "\n",
    "# We set the embedding layer's parameters from GloVe\n",
    "model.embedding.weight.data.copy_(torch.tensor(embedding_matrix))\n",
    "# We won't change/train the embedding layer\n",
    "model.embedding.weight.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will define the optimizer and loss function below. __Binary cross-entropy loss__ is used as this is a binary classification problem.\n",
    "$$\n",
    "\\mathrm{BinaryCrossEntropyLoss} = -\\sum_{examples}{(y\\log(p) + (1 - y)\\log(1 - p))}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Setting our optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# We will use Binary Cross-entropy loss\n",
    "criterion = torch.nn.BCELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, it is time to start the training process. We will print the Binary cross-entropy loss after each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "for epoch in range(epochs):\n",
    "    start = time.time()\n",
    "    model.train()\n",
    "    training_loss = 0.0\n",
    "    # Training loop\n",
    "    for data, target in train_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data.to(torch.long))\n",
    "        loss = criterion(output.squeeze(), target.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        training_loss += loss.item() * data.size(0)\n",
    "    \n",
    "    # Calculate validation loss\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_predictions = model(torch.tensor(val_text_transformed).to(device).to(torch.long))\n",
    "        val_loss = criterion(val_predictions.squeeze(), val_label.float().to(device))\n",
    "    \n",
    "    # Let's take the average losses\n",
    "    training_loss = training_loss / len(train_label)\n",
    "    val_loss = val_loss.item()\n",
    "    \n",
    "    end = time.time()\n",
    "    print(f\"Epoch {epoch}. Train_loss {training_loss:.4f} Validation_loss {val_loss:.4f} Seconds {end-start:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see some validation results below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Get validation predictions\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    val_predictions = model(torch.tensor(val_text_transformed).to(device).to(torch.long))\n",
    "\n",
    "# Round predictions: 1 if pred>0.5, 0 otherwise\n",
    "val_predictions = (val_predictions.squeeze().cpu() > 0.5).float().numpy()\n",
    "\n",
    "print(\"Classification Report\")\n",
    "print(classification_report(val_label.numpy(), val_predictions))\n",
    "print(\"Accuracy\")\n",
    "print(accuracy_score(val_label.numpy(), val_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. <a name=\"7\">Improvement ideas</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "We can improve our model by\n",
    "* Changing hyper-parameters\n",
    "* Using more advanced architectures such as Gated Recurrent Units (GRU) and Long Short-term Memory networks (LSTM)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
