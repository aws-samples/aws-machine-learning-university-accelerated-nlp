{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![MLU Logo](../data/MLU_Logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name=\"0\">Machine Learning Accelerator - Natural Language Processing - Lecture 3</a>\n",
    "\n",
    "## Neural Networks with PyTorch\n",
    "\n",
    "In this notebook, we will build, train and validate a Neural Network using PyTorch.\n",
    "1. <a href=\"#1\">Implementing a neural network with PyTorch</a>\n",
    "2. <a href=\"#2\">Loss Functions</a>\n",
    "3. <a href=\"#3\">Training</a>\n",
    "4. <a href=\"#4\">Example - Binary Classification</a>\n",
    "5. <a href=\"#5\">Natural Language Processing Context</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install -q -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. <a name=\"1\">Implementing a neural network with PyTorch</a>\n",
    "(<a href=\"#0\">Go to top</a>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's implement a simple neural network with two hidden layers of size 64 and 128 using the sequential mode (Adding things in sequence). We will have 3 inputs, 2 hidden layers and 1 output layer. Some drop-outs attached to the hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(3, 64),                # Linear layer-1 with 64 units\n",
    "    nn.Tanh(),                       # Tanh activation is applied\n",
    "    nn.Dropout(0.4),                 # Apply random 40% drop-out to layer_1\n",
    "    \n",
    "    nn.Linear(64, 64),               # Linear layer-2 with 64 units\n",
    "    nn.Tanh(),                       # Tanh activation is applied\n",
    "    nn.Dropout(0.3),                 # Apply random 30% drop-out to layer_2\n",
    "    \n",
    "    nn.Linear(64, 1)                 # Output layer with single unit\n",
    ")\n",
    "\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can initialize the weights of the network with 'apply()' function. We prefer to use the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "\n",
    "net.apply(init_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at our layers and dropouts on them. We can easily access them with net[layer_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(net[0])\n",
    "print(net[1])\n",
    "print(net[2])\n",
    "print(net[3])\n",
    "print(net[4])\n",
    "print(net[5])\n",
    "print(net[6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. <a name=\"2\">Loss Functions</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "We will go over some popular loss functions here. We can select loss functions according to our problem. Full list of supported loss functions are available [here](https://pytorch.org/docs/stable/nn.html#loss-functions)\n",
    "\n",
    "\n",
    "__Binary Cross-entropy Loss:__ A common loss function for binary classification. It is given by: \n",
    "$$\n",
    "\\mathrm{BinaryCrossEntropyLoss} = -\\sum_{examples}{(y\\log(p) + (1 - y)\\log(1 - p))}\n",
    "$$\n",
    "where p is the prediction (between 0 and 1, ie. 0.831) and y is the true class (either 1 or 0). \n",
    "\n",
    "In PyTorch, we can use binary cross entropy with `BCEWithLogitsLoss`. It also applies sigmoid function on the predictions. Therefore, p is always between 0 and 1.\n",
    "\n",
    "\n",
    "```python\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "loss = BCEWithLogitsLoss()\n",
    "```\n",
    "__Categorical Cross-entropy Loss:__ It is used for multi-class classification. We apply the softmax function on prediction probabilities and then extend the equation of binary cross-entropy. After the softmax function, summation of the predictions are equal to 1. Equation is below. y becomes 1 for true class and 0 for other classes.\n",
    "$$\n",
    "\\mathrm{CategoricalCrossEntropyLoss} = -\\sum_{examples}\\sum_{classes}{y_j\\log(p_j)}\n",
    "$$\n",
    "In PyTorch, `CrossEntropyLoss` implements the categorical cross-entropy loss with softmax function\n",
    "\n",
    "\n",
    "```python\n",
    "from torch.nn import CrossEntropyLoss\n",
    "loss = CrossEntropyLoss()\n",
    "```\n",
    "__MSE Loss:__ This is a loss function for regression problems. It measures the squared difference between target values (y) and predictions (p). Here, square makes sure the offsets with different signs don't cancel out each other.\n",
    "$$\n",
    "\\mathrm{MSE loss} = \\frac{1}{n} \\sum_{examples}(y - p)^2\n",
    "$$\n",
    "In PyTorch, we can use it with `MSELoss`:\n",
    "```python\n",
    "from torch.nn import MSELoss\n",
    "loss = MSELoss()\n",
    "```\n",
    "__L1 Loss:__ This is similar to MSE loss. It measures the absolute difference between target values (y) and predictions (p).\n",
    "$$\n",
    "\\mathrm{L1 loss} = \\frac{1}{n} \\sum_{examples}|y - p|\n",
    "$$\n",
    "In PyTorch, we can use it with `L1Loss`:\n",
    "```python\n",
    "from torch.nn import L1Loss\n",
    "loss = L1Loss()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. <a name=\"3\">Training</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "`torch.optim` module provides necessary training algorithms for neural networks. We can use the following for training a network using Stochastic Gradient Descent method and learning rate of 0.001.\n",
    "\n",
    "```python\n",
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. <a name=\"4\">Example - Binary Classification</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "Let's train a neural network on a random dataset. We have two classes and will learn to classify them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_circles\n",
    "\n",
    "X, y = make_circles(n_samples=750, shuffle=True, random_state=42, noise=0.05, factor=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_dataset(X, y, title):\n",
    "    \n",
    "    # Activate Seaborn visualization\n",
    "    sns.set()\n",
    "    \n",
    "    # Plot both classes: Class1->Blue, Class2->Red\n",
    "    plt.scatter(X[y==1, 0], X[y==1, 1], c='blue', label=\"class 1\")\n",
    "    plt.scatter(X[y==0, 0], X[y==0, 1], c='red', label=\"class 2\")\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.xlabel('x1')\n",
    "    plt.ylabel('x2')\n",
    "    plt.xlim(-2, 2)\n",
    "    plt.ylim(-2, 2)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "    \n",
    "plot_dataset(X, y, title=\"Dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are creating the network below. We will have two hidden layers. Since the data seems easily separable, we can have a small network (2 hidden layers) with 10 units at each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential(\n",
    "    nn.Linear(2, 10),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(10, 10),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(10, 1),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "\n",
    "net.apply(init_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4           # How many samples to use for each weight update \n",
    "epochs = 50              # Total number of iterations\n",
    "learning_rate = 0.01     # Learning rate\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Using GPU if available\n",
    "\n",
    "# Define the loss. As we used sigmoid in the last layer, use BCELoss\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Define the optimizer, SGD with learning rate\n",
    "optimizer = optim.SGD(net.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into two parts: 80%-20% split\n",
    "X_train, X_val = X[:int(len(X)*0.8), :], X[int(len(X)*0.8):, :]\n",
    "y_train, y_val = y[:int(len(X)*0.8)], y[int(len(X)*0.8):]\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train = torch.FloatTensor(X_train)\n",
    "X_val = torch.FloatTensor(X_val)\n",
    "y_train = torch.FloatTensor(y_train).unsqueeze(1)\n",
    "y_val = torch.FloatTensor(y_val).unsqueeze(1)\n",
    "\n",
    "# Using PyTorch DataLoader to load the data in batches\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start the training process. We will have training and validation sets and print our losses at each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "for epoch in range(epochs):\n",
    "    start = time.time()\n",
    "    training_loss = 0\n",
    "    # Training loop, train the network\n",
    "    net.train()\n",
    "    for idx, (data, target) in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = net(data)\n",
    "        loss = criterion(output, target)\n",
    "        training_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Get validation predictions\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        val_predictions = net(X_val.to(device))\n",
    "        # Calculate validation loss\n",
    "        val_loss = criterion(val_predictions, y_val.to(device)).item()\n",
    "    \n",
    "    # Let's take the average losses\n",
    "    training_loss = training_loss / len(y_train)\n",
    "    val_loss = val_loss / len(y_val)\n",
    "    \n",
    "    train_losses.append(training_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    end = time.time()\n",
    "    print(f\"Epoch {epoch}. Train_loss {training_loss:.6f} Validation_loss {val_loss:.6f} Seconds {end-start:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the training and validation loss plots below. Losses go down as the training process continues as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.plot(train_losses, label=\"Training Loss\")\n",
    "plt.plot(val_losses, label=\"Validation Loss\")\n",
    "plt.title(\"Loss values\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. <a name=\"5\">Natural Language Processing Context</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "If we want to use the same type of architecture for text classification, we need to apply some feature extraction methods first. For example: We can get TF-IDF vectors of text fields. After that, we can use neural networks on those features. \n",
    "\n",
    "We will also look at __more advanced neural network architrectures__ such as __Recurrent Neural Networks (RNNs)__, __Long Short-Term Memory networks (LSTMs)__ and __Transformers__. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
